--- sys/kern/sched_ule.c	2015-02-19 07:59:27.626127250 -0800
+++ sys/kern/sched_debug.c	2015-02-20 23:26:22.501581000 -0800
@@ -1,4 +1,12 @@
 /*-
+ * Copyright (c) 2015, Harrison Grundy <harrison.grundy@astrodoggroup.com>
+ * Based entirely on sched_ule.c from Jeffrey Roberson. Any mistakes are
+ * likely to be mine. This scheduler is supposed to create edge cases in
+ * scheduling. As a result, it may destroy your data, render your machine
+ * unbootable and corrupt data on any device attached to it.
+ */
+
+/*-
  * Copyright (c) 2002-2007, Jeffrey Roberson <jeff@freebsd.org>
  * All rights reserved.
  *
@@ -25,20 +33,14 @@
  */
 
 /*
- * This file implements the ULE scheduler.  ULE supports independent CPU
- * run queues and fine grain locking.  It has superior interactive
- * performance under load even on uni-processor systems.
- *
- * etymology:
- *   ULE is the last three letters in schedule.  It owes its name to a
- * generic user created for a scheduling system by Paul Mikesell at
- * Isilon Systems and a general lack of creativity on the part of the author.
+ * This file implements the debugging scheduler. It's designed to break 
+ * things. Beware.
  */
 
 #include <sys/cdefs.h>
-__FBSDID("$FreeBSD: head/sys/kern/sched_ule.c 273266 2014-10-18 19:36:11Z adrian $");
 
 #include "opt_hwpmc_hooks.h"
+#include "opt_kdtrace.h"
 #include "opt_sched.h"
 
 #include <sys/param.h>
@@ -46,7 +48,6 @@
 #include <sys/kdb.h>
 #include <sys/kernel.h>
 #include <sys/ktr.h>
-#include <sys/limits.h>
 #include <sys/lock.h>
 #include <sys/mutex.h>
 #include <sys/proc.h>
@@ -63,6 +64,7 @@
 #include <sys/vmmeter.h>
 #include <sys/cpuset.h>
 #include <sys/sbuf.h>
+#include <sys/limits.h>
 
 #ifdef HWPMC_HOOKS
 #include <sys/pmckern.h>
@@ -191,6 +193,16 @@
 #define	SCHED_SLICE_DEFAULT_DIVISOR	10	/* ~94 ms, 12 stathz ticks. */
 #define	SCHED_SLICE_MIN_DIVISOR		6	/* DEFAULT/MIN = ~16 ms. */
 
+static int sched_slice_minticks = 2;
+static int sched_slice_maxticks = 12;
+static int sched_priority_random = 0;
+static int sched_alg = 0; /* Use ULE standard algorithm by default */
+static int sched_utilticks = 10000; /* Time for CPU Usage Averaging */
+static int sched_utilticksmax = 11000;
+static int sched_balancenum = 1; /* Max threads in balance_pair */
+
+
+
 /* Flags kept in td_flags. */
 #define	TDF_SLICEEND	TDF_SCHED2	/* Thread time slice is over. */
 
@@ -578,11 +590,11 @@
 	 * cannot be higher priority load in the system.
 	 */
 	load = tdq->tdq_sysload - 1;
-	if (load >= SCHED_SLICE_MIN_DIVISOR)
-		return (sched_slice_min);
+	if (load >= sched_slice_minticks)
+		return (sched_slice_minticks);
 	if (load <= 1)
-		return (sched_slice);
-	return (sched_slice / load);
+		return (sched_slice_maxticks);
+	return (sched_slice_maxticks / load);
 }
 
 /*
@@ -861,14 +873,15 @@
 {
 	struct tdq *tdq;
 
+	if (smp_started == 0 || rebalance == 0)
+		return;	
 	/*
 	 * Select a random time between .5 * balance_interval and
 	 * 1.5 * balance_interval.
 	 */
+	
 	balance_ticks = max(balance_interval / 2, 1);
 	balance_ticks += random() % balance_interval;
-	if (smp_started == 0 || rebalance == 0)
-		return;
 	tdq = TDQ_SELF();
 	TDQ_UNLOCK(tdq);
 	sched_balance_group(cpu_top);
@@ -908,25 +921,67 @@
 {
 	int moved;
 	int cpu;
-
-	tdq_lock_pair(high, low);
+	
 	moved = 0;
-	/*
-	 * Determine what the imbalance is and then adjust that to how many
-	 * threads we actually have to give up (transferable).
-	 */
-	if (high->tdq_transferable != 0 && high->tdq_load > low->tdq_load &&
-	    (moved = tdq_move(high, low)) > 0) {
+	
+	/* ULE Standard Behavior. Move 1 thread. */
+	
+	if (sched_balancenum == 1) {
+	
+		tdq_lock_pair(high, low);
 		/*
-		 * In case the target isn't the current cpu IPI it to force a
-		 * reschedule with the new workload.
-		 */
-		cpu = TDQ_ID(low);
-		if (cpu != PCPU_GET(cpuid))
-			ipi_cpu(cpu, IPI_PREEMPT);
+		* Determine what the imbalance is and then adjust that to how many
+		* threads we actually have to give up (transferable).
+		*/
+	
+		if (high->tdq_transferable != 0 && high->tdq_load > low->tdq_load &&
+			(moved = tdq_move(high, low)) > 0) {
+			/*
+			* In case the target isn't the current cpu IPI it to force a
+			* reschedule with the new workload.
+			*/
+			cpu = TDQ_ID(low);
+			if (cpu != PCPU_GET(cpuid))
+				ipi_cpu(cpu, IPI_PREEMPT);
+		}
+		tdq_unlock_pair(high, low);
 	}
-	tdq_unlock_pair(high, low);
-	return (moved);
+	
+	/* DEBUG Sysctl tweaked. Move as many threads as specified. */
+
+	else {
+		TDQ_LOCK(high);
+		TDQ_LOCK_FLAGS(high, MTX_DUPOK);
+		while (moved < sched_balancenum ) {
+			
+			/* See if we have anything to migrate. */
+			
+			if (high->tdq_transferable != 0 && high->tdq_load > low->tdq_load) {
+				TDQ_LOCK(low);
+				TDQ_LOCK_FLAGS(low, MTX_DUPOK);
+				if ((moved = moved + tdq_move(high, low)) > 0) {
+					TDQ_UNLOCK(low);
+				}
+				else {
+					
+					/* We were unsuccessful. Nothing left to do. */
+					
+					TDQ_UNLOCK(low);
+					break;
+				}
+			}
+			else {
+				break;
+			}
+		}
+		TDQ_UNLOCK(high);
+		if (moved) {
+			cpu = TDQ_ID(low);
+			if (cpu != PCPU_GET(cpuid))
+				ipi_cpu(cpu, IPI_PREEMPT);
+		}
+	}
+	return(moved);
 }
 
 /*
@@ -1039,8 +1094,8 @@
 		return;
 
 	/*
-	 * Make sure that our caller's earlier update to tdq_load is
-	 * globally visible before we read tdq_cpu_idle.  Idle thread
+	 * Make sure that tdq_load updated before calling this function
+	 * is globally visible before we read tdq_cpu_idle.  Idle thread
 	 * accesses both of them without locks, and the order is important.
 	 */
 	mb();
@@ -1084,11 +1139,11 @@
 				continue;
 			rqh = &rq->rq_queues[bit + (i << RQB_L2BPW)];
 			TAILQ_FOREACH(td, rqh, td_runq) {
-				if (first && THREAD_CAN_MIGRATE(td) &&
+				if (first && THREAD_CAN_MIGRATE(td) && 
 				    THREAD_CAN_SCHED(td, cpu))
 					return (td);
 				first = td;
-			}
+			}	
 		}
 	}
 	if (start != 0) {
@@ -1499,6 +1554,9 @@
 	 * considered interactive.
 	 */
 	score = imax(0, sched_interact_score(td) + td->td_proc->p_nice);
+	if (sched_priority_random > 0) 
+		score = score + ((random() % (sched_priority_random*2)) - sched_priority_random);	
+
 	if (score < sched_interact) {
 		pri = PRI_MIN_INTERACT;
 		pri += ((PRI_MAX_INTERACT - PRI_MIN_INTERACT + 1) /
@@ -1629,13 +1687,13 @@
 {
 	int t = ticks;
 
-	if (t - ts->ts_ltick >= SCHED_TICK_TARG) {
+	if (t - ts->ts_ltick >= sched_utilticks) {
 		ts->ts_ticks = 0;
-		ts->ts_ftick = t - SCHED_TICK_TARG;
-	} else if (t - ts->ts_ftick >= SCHED_TICK_MAX) {
+		ts->ts_ftick = t - sched_utilticks;
+	} else if (t - ts->ts_ftick >= sched_utilticksmax) {
 		ts->ts_ticks = (ts->ts_ticks / (ts->ts_ltick - ts->ts_ftick)) *
-		    (ts->ts_ltick - (t - SCHED_TICK_TARG));
-		ts->ts_ftick = t - SCHED_TICK_TARG;
+		    (ts->ts_ltick - (t - sched_utilticks));
+		ts->ts_ftick = t - sched_utilticks;
 	}
 	if (run)
 		ts->ts_ticks += (t - ts->ts_ltick) << SCHED_TICK_SHIFT;
@@ -2392,13 +2450,26 @@
 	 * Pick the destination cpu and if it isn't ours transfer to the
 	 * target cpu.
 	 */
-	cpu = sched_pickcpu(td, flags);
-	tdq = sched_setcpu(td, cpu, flags);
-	tdq_add(tdq, td, flags);
-	if (cpu != PCPU_GET(cpuid)) {
-		tdq_notify(tdq, td);
-		return;
-	}
+	switch (sched_alg) {
+	case 1: 
+		cpu = random() % mp_maxid;
+	 	tdq = sched_setcpu(td, cpu, flags);
+		tdq_add(tdq, td, flags);
+		if (cpu != PCPU_GET(cpuid)) {
+			tdq_notify(tdq, td);
+			return;
+		}	
+		break;
+	default: 
+		cpu = sched_pickcpu(td, flags);
+        tdq = sched_setcpu(td, cpu, flags);
+        tdq_add(tdq, td, flags);
+			if (cpu != PCPU_GET(cpuid)) {
+				tdq_notify(tdq, td);
+                return;
+			}
+		break;
+	}	
 #else
 	tdq = TDQ_SELF();
 	TDQ_LOCK(tdq);
@@ -2510,20 +2581,16 @@
 	ts = td->td_sched;
 	if (ts->ts_flags & TSF_BOUND)
 		sched_unbind(td);
-	if (PCPU_GET(cpuid) == cpu) {
-		ts->ts_flags |= TSF_BOUND;	
-		sched_pin();
+	KASSERT(THREAD_CAN_MIGRATE(td), ("%p must be migratable", td));
+	ts->ts_flags |= TSF_BOUND;
+	sched_pin();
+	if (PCPU_GET(cpuid) == cpu)
 		return;
-	}
-	else {	
-		KASSERT(THREAD_CAN_MIGRATE(td), ("%p must be migratable", td));
-		ts->ts_flags |= TSF_BOUND;
-		sched_pin();
-		ts->ts_cpu = cpu;
-		/* When we return from mi_switch we'll be on the correct cpu. */
-		mi_switch(SW_VOL, NULL);
-	}
+	ts->ts_cpu = cpu;
+	/* When we return from mi_switch we'll be on the correct cpu. */
+	mi_switch(SW_VOL, NULL);
 }
+
 /*
  * Release a bound thread.
  */
@@ -2856,7 +2923,7 @@
 }
 
 SYSCTL_NODE(_kern, OID_AUTO, sched, CTLFLAG_RW, 0, "Scheduler");
-SYSCTL_STRING(_kern_sched, OID_AUTO, name, CTLFLAG_RD, "ULE", 0,
+SYSCTL_STRING(_kern_sched, OID_AUTO, name, CTLFLAG_RD, "DEBUG", 0,
     "Scheduler name");
 SYSCTL_PROC(_kern_sched, OID_AUTO, quantum, CTLTYPE_INT | CTLFLAG_RW,
     NULL, 0, sysctl_kern_quantum, "I",
@@ -2875,6 +2942,14 @@
 SYSCTL_INT(_kern_sched, OID_AUTO, idlespinthresh, CTLFLAG_RW,
     &sched_idlespinthresh, 0,
     "Threshold before we will permit idle thread spinning");
+SYSCTL_INT(_kern_sched, OID_AUTO, maxslice, CTLFLAG_RW,
+    &sched_slice_maxticks, 0, "Max Slice Size in Ticks");
+SYSCTL_INT(_kern_sched, OID_AUTO, minslice, CTLFLAG_RW, &sched_slice_minticks, 0, "Min Slice Size in Ticks");
+SYSCTL_INT(_kern_sched, OID_AUTO, priority_random, CTLFLAG_RW, &sched_priority_random, 0, "Thread Priority Randomizer");
+SYSCTL_INT(_kern_sched, OID_AUTO, algorithm, CTLFLAG_RW, &sched_alg, 0, "Scheduler Algorithm Selector");
+SYSCTL_INT(_kern_sched, OID_AUTO, utilticks, CTLFLAG_RW, &sched_utilticks, 0, "CPU Usage Calculation Interval in ticks"); 
+SYSCTL_INT(_kern_sched, OID_AUTO, utilticksmax, CTLFLAG_RW, &sched_utilticksmax, 0, "CPU Usage Maximum Calculation Interval in ticks");
+SYSCTL_INT(_kern_sched, OID_AUTO, tdbalancenum, CTLFLAG_RW, &sched_balancenum, 0, "Number of threads to attempt to balance with.");
 #ifdef SMP
 SYSCTL_INT(_kern_sched, OID_AUTO, affinity, CTLFLAG_RW, &affinity, 0,
     "Number of hz ticks to keep thread affinity for");
